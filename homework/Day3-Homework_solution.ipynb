{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Homework Exercises\n",
    "\n",
    "This material provides some hands-on experience using the methods learned from the third day's material."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "import pymc as pm\n",
    "import arviz as az"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Effects of coaching on SAT scores\n",
    "\n",
    "This example was taken from Gelman *et al.* (2013):\n",
    "\n",
    "> A study was performed for the Educational Testing Service to analyze the effects of special coaching programs on test scores. Separate randomized experiments were performed to estimate the effects of coaching programs for the SAT-V (Scholastic Aptitude Test- Verbal) in each of eight high schools. The outcome variable in each study was the score on a special administration of the SAT-V, a standardized multiple choice test administered by the Educational Testing Service and used to help colleges make admissions decisions; the scores can vary between 200 and 800, with mean about 500 and standard deviation about 100. The SAT examinations are designed to be resistant to short-term efforts directed specifically toward improving performance on the test; instead they are designed to reflect knowledge acquired and abilities developed over many years of education. Nevertheless, each of the eight schools in this study considered its short-term coaching program to be successful at increasing SAT scores. Also, there was no prior reason to believe that any of the eight programs was more effective than any other or that some were more similar in effect to each other than to any other.\n",
    "\n",
    "You are given the estimated coaching effects (`d`) and their sampling variances (`s`). The estimates were obtained by independent experiments, with relatively large sample sizes (over thirty students in each school), so you can assume that they have approximately normal sampling distributions with known variances variances.\n",
    "\n",
    "Here are the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = 8\n",
    "d = np.array([28.,  8., -3.,  7., -1.,  1., 18., 12.])\n",
    "s = np.array([15., 10., 16., 11.,  9., 11., 10., 18.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct an appropriate model for estimating whether coaching effects are positive, using a **centered parameterization**, and then compare the diagnostics for this model to that from an **uncentered parameterization**.\n",
    "\n",
    "Finally, perform goodness-of-fit diagnostics on the better model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schools = np.array(\n",
    "    [\n",
    "        \"Choate\",\n",
    "        \"Deerfield\",\n",
    "        \"Phillips Andover\",\n",
    "        \"Phillips Exeter\",\n",
    "        \"Hotchkiss\",\n",
    "        \"Lawrenceville\",\n",
    "        \"St. Paul's\",\n",
    "        \"Mt. Hermon\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "with pm.Model(coords={'school': schools}) as schools_centered:\n",
    "    \n",
    "    mu = pm.Normal(\"mu\", 0, sigma=1e6)\n",
    "    tau = pm.HalfCauchy(\"tau\", 5)\n",
    "\n",
    "    theta = pm.Normal(\"theta\", mu, sigma=tau, dims='school')\n",
    "\n",
    "    obs = pm.Normal(\"obs\", theta, sigma=s, observed=d)\n",
    "\n",
    "    trace_centered = pm.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model(coords={'school': schools}) as schools_noncentered:\n",
    "    \n",
    "    mu = pm.Normal(\"mu\", 0, sigma=1e6)\n",
    "    tau = pm.HalfCauchy(\"tau\", 5)\n",
    "\n",
    "    z = pm.Normal(\"z\", 0, sigma=1, dims='school')\n",
    "    theta = mu + z * tau\n",
    "\n",
    "    obs = pm.Normal(\"obs\", theta, sigma=s, observed=d)\n",
    "\n",
    "    trace_noncentered = pm.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with schools_centered:\n",
    "    pm.compute_log_likelihood(trace_centered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with schools_noncentered:\n",
    "    pm.compute_log_likelihood(trace_noncentered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.compare({'centered': trace_centered, 'non-centered': trace_noncentered}, ic='loo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with schools_centered:\n",
    "    pm.sample_posterior_predictive(trace_centered, extend_inferencedata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_ppc(trace_centered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Car Price Prediction\n",
    "\n",
    "We will use a small subset of [this Kaggle dataset](https://www.kaggle.com/datasets/nehalbirla/vehicle-dataset-from-cardekho). This data is amenable to both multiple linear regression and hierarchical regression, you can start with the former and move on to the latter. The data has the selling price of multiple used cars, along with some other features such as year of production, kilometers driven, fuel type, car brand and model, etc.\n",
    "\n",
    "The full dataset has a lot of information, but we chose to crop it down to a simpler form that is easier for you to work with. Our reduced dataset has the following features:\n",
    "\n",
    "- Selling price in thousands of dollars\n",
    "- Natural log of the kilometers driven\n",
    "- Year of production\n",
    "- Number of seats\n",
    "- Brand name\n",
    "- Car name\n",
    "- Full name of the car (with special edition modifiers)\n",
    "- Kilometers per liter of fuel\n",
    "- Engine CC\n",
    "\n",
    "The goal of this exercise is to be able to **predict the selling price based on the rest of the features using linear regressions**. We propose that you do the following steps to do so:\n",
    "\n",
    "1. Do an explorative data analysis. Visualize the distribution of selling prices and how it relates to the other features. Look at how the distribution changes across brands. Try to reason whether it's appropriate to transform the observations in some way to ellicit a linear relationship with some features in the dataset.\n",
    "2. Assuming the observations are normally distributed, write a series of linear regression models to fit the dataset\n",
    "    a. Try a model with a single intercept parameter and do prior predictive analysis to determine appropriate priors.\n",
    "    b. Add linear terms that relate the model with the numerical features in the data. Try to reason about adequate priors for the slopes, and whether it helps to standardize the features or not.\n",
    "    c. Run inference on both models and explain what happens to the observational noise parameter.\n",
    "3. Add a hierarchical structure that depends on the car brand. Do prior predictive simulations to choose appropriate priors, and run inference. The hierarchical structure could be on the intercept term, the slopes or both.\n",
    "4. (HARD) Add a nested hierarchy term to the model's intercept. The first level of the hierarchy is the brand, the second layer is the car name. Explore what happens if you use the centered or the non-centered parametrization in the second layer of the hierarchy.\n",
    "5. Compare all of the above models using `arviz.compare`. Which one is the highest ranking predictive model?\n",
    "6. (VERY HARD, Optional and without a coded answer) This is for people that want to explore beyond what we did with a simple linear regression. All of the models we used assumed normally distributed observations. The best fitting model is unable to explain the observed distribution of prices, and the reason for this is that prices are not normally distributed around a mean. Prices are chosen as multiple of a thousand, and usually they are grouped into tiers. It's not unusual to have multiple cars sold at exactly the same price. This leads to heteroskedastic observations across brands and car names, and the assumption of a unique observational noise parameter breaks down. We don't have a concrete answer on what observational distribution should really underlie the observations. We want to invite you to try to think:\n",
    "    a. How could you have different observational noises while still assuming that the observations are normally distributed?\n",
    "    b. What kind of observational distribution could you use to account for the groupings of multiple selling prices at the same value, and then some other selling prices that have more dispersion? As a hint, try to look into mixture models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import arviz as az\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from pytensor import tensor as pt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sample_kwargs = {\n",
    "    \"random_seed\": 42,\n",
    "    \"chains\": 4,\n",
    "    \"idata_kwargs\": {\"log_likelihood\": True},\n",
    "}\n",
    "\n",
    "plt.style.use(\"arviz-darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(\"..\", \"data\", \"reduced_car_data.csv\"), header=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df, hue=\"brand\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(\"..\", \"data\", \"reduced_car_data.csv\"), header=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    # We ignore the warnings that seaborn is changing the layout of the plotting style\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this data, we can clearly see two things.\n",
    "\n",
    "1. Selling price is positive and concentrated at low values with potentially long tails\n",
    "2. There seems to be a strong non linear relation between selling price and year\n",
    "\n",
    "The strong relationship between selling price and year makes sense. It's very common to say that an old used car is worth less than a new used car. To make sense about the non-linearity let's have a look at the distribution of selling price.\n",
    "\n",
    "People reason strangely about numbers and values. They tend to think about orders of magnitude instead of the actual number that is presented to determine if one thing is more valuable than another. Since consumers will have this at their core, it's also natural that sellers will set prices based on order of magnitude considerations more than on actual monetary value or costs of a car. If this were true, taking the logarithm of the selling price should lead to a distribution that is less skewed and to a clearer linear relationship between year and price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[[\"year\"]].assign(log_selling_price=np.log(df.selling_price))\n",
    "with warnings.catch_warnings():\n",
    "    # We ignore the warnings that seaborn is changing the layout of the plotting style\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    sns.pairplot(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks much better now.\n",
    "- The selling price is not completely normal, but it is less skewed than before\n",
    "- The relationship between the log of selling price and year seems linear, albeit with a lot of dispersion (potentially due to other factors like km driven or engine CC)\n",
    "\n",
    "Let's now look at the whole pairplot when we use the log of selling price. We will also standardize the numerical features. The reason we do this is that a linear regression where the features have non-zero mean, will affect the estimated intercept. In turn, this will make it harder to set a prior on the intercept when adding in new features to the regression. Furthermore, if the features are not rescaled, it would be necessary to set a different prior for each feature's slope parameter, to make the model assume that all features could be equally weighted a priori."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "features = [\"year\", \"log_km_driven\", \"kmpl\", \"engine_cc\"]\n",
    "categoricals = [\"name\", \"brand\", \"seats\"]\n",
    "categories = {k: pd.factorize(df[k], sort=True) for k in categoricals}\n",
    "brands = list(categories[\"brand\"][1])\n",
    "name_to_brand_idx = {\n",
    "    i: brands.index(n.split(\" \")[0]) for i, n in enumerate(categories[\"name\"][1])\n",
    "}\n",
    "observed = \"selling_price\"\n",
    "df2 = pd.DataFrame(\n",
    "    scaler.fit_transform(df[features]),\n",
    "    columns=features,\n",
    ")\n",
    "df2 = df2.assign(\n",
    "    log_selling_price=np.log(df[observed]),\n",
    "    **{k: df[k] for k in categoricals},\n",
    ")\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One other thing that we can say is that number of seats is a rather bad categorical feature. Almost all of the observations were taken for 5 seat vehicles, and there doesn't seem to be much structure for the other seats. We won't add it as a regressor via a slope, but we could consider it as an added intercept that depends on the number of seats. For now, we will simply ignore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    # We ignore the warnings that seaborn is changing the layout of the plotting style\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    sns.pairplot(df2, hue=\"seats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's have a look at how brand structures the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    # We ignore the warnings that seaborn is changing the layout of the plotting style\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    sns.pairplot(df2, hue=\"brand\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a bit hard to disentangle what is going on here, but it seems that brand influences selling price a lot. It's not clear if it does so mediated by some of the other features in the dataset, if it directly offsets the selling price, or both. Let's simply focus on the distribution of selling price per brand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(df2.log_selling_price.min(), df2.log_selling_price.max(), 31)\n",
    "colors = [\n",
    "    plt.get_cmap(\"magma\")(x) for x in np.linspace(0, 1, len(categories[\"brand\"][1]))\n",
    "]\n",
    "nrows = np.ceil(len(categories[\"brand\"][1]) / 4)\n",
    "fig, axs = plt.subplots(int(nrows), 4, figsize=(7, nrows * 1.5), layout=\"constrained\")\n",
    "for brand, color, ax in zip(categories[\"brand\"][1], colors, axs.flatten()):\n",
    "    subdf = df2.query(\"brand == @brand\")\n",
    "    ax.hist(subdf.log_selling_price.values, bins=bins, density=True, color=color)\n",
    "    ax.set_title(brand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the brand influences the distribution of selling prices. It seems like the mean is shifted across brands and the dispersion is different across brands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(df2.log_selling_price.min(), df2.log_selling_price.max(), 31)\n",
    "fig, axs = plt.subplots(\n",
    "    len(categories[\"brand\"][1]), 5, figsize=(14, 20), layout=\"constrained\"\n",
    ")\n",
    "for axs_row, (brand, subdf) in zip(axs, df2.groupby(\"brand\")):\n",
    "    for ax, (name, by_name) in itertools.zip_longest(\n",
    "        axs_row, subdf.groupby(\"name\"), fillvalue=(None, None)\n",
    "    ):\n",
    "        if isinstance(ax, tuple):\n",
    "            break\n",
    "        if by_name is None:\n",
    "            ax.set_axis_off()\n",
    "        else:\n",
    "            ax.hist(by_name.log_selling_price.values, bins=bins, density=True)\n",
    "            ax.set_title(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at individual car models, the story seems to be the same as for the overall brand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear models\n",
    "\n",
    "The first model simply estimates the mean selling price and observational noise, and it is useful for setting priors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model(coords={\"obs_idx\": df2.index}) as m0:\n",
    "    intercept = pm.Normal(\"intercept\", 6, 1)\n",
    "    mu = intercept\n",
    "    sigma = pm.Exponential(\"sigma\", lam=1)\n",
    "    pm.Normal(\"log_price\", mu, sigma, observed=df2.log_selling_price, dims=\"obs_idx\")\n",
    "    idata = i0 = pm.sample(**sample_kwargs)\n",
    "    idata.extend(pm.sample_prior_predictive())\n",
    "    idata.extend(pm.sample_posterior_predictive(idata))\n",
    "az.plot_trace(idata);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_ppc(i0, group=\"prior\", num_pp_samples=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_ppc(i0, num_pp_samples=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = {\"feature\": features, \"obs_idx\": df2.index}\n",
    "with pm.Model(coords=coords) as m1:\n",
    "    intercept = pm.Normal(\"intercept\", 6, 1)\n",
    "    slopes = pm.Normal(\"slopes\", 0, 0.3, dims=\"feature\")\n",
    "    mu = intercept + pm.math.dot(df2[coords[\"feature\"]].values, slopes)\n",
    "    sigma = pm.Exponential(\"sigma\", lam=1)\n",
    "    pm.Normal(\"log_price\", mu, sigma, observed=df2.log_selling_price, dims=\"obs_idx\")\n",
    "    idata = i1 = pm.sample(**sample_kwargs)\n",
    "    idata.extend(pm.sample_prior_predictive())\n",
    "    idata.extend(pm.sample_posterior_predictive(idata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_ppc(idata, group=\"prior\", num_pp_samples=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(idata);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_ppc(idata, num_pp_samples=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical brand models\n",
    "\n",
    "Now, we will look into the hierarchical structure for brands. We will try to use a centered parametrization for the intercept variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = {\n",
    "    \"feature\": features,\n",
    "    \"brand\": categories[\"brand\"][1],\n",
    "    \"obs_idx\": df2.index,\n",
    "}\n",
    "with pm.Model(coords=coords) as m2:\n",
    "    idx = categories[\"brand\"][0]\n",
    "    intercept_global = pm.Normal(\"intercept_global\", 6, 1)\n",
    "    intercept_brand_scale = pm.HalfNormal(\"intercept_brand_scale\", 0.5)\n",
    "    intercept = pm.Normal(\n",
    "        \"intercept\", intercept_global, intercept_brand_scale, dims=\"brand\"\n",
    "    )\n",
    "    slopes = pm.Normal(\"slopes\", 0, 0.1, dims=\"feature\")\n",
    "    mu = intercept[idx] + pm.math.dot(df2[coords[\"feature\"]].values, slopes)\n",
    "    sigma = pm.Exponential(\"sigma\", lam=1)\n",
    "    pm.Normal(\"log_price\", mu, sigma, observed=df2.log_selling_price, dims=\"obs_idx\")\n",
    "    idata = i2 = pm.sample(**sample_kwargs)\n",
    "    idata.extend(pm.sample_prior_predictive())\n",
    "    idata.extend(pm.sample_posterior_predictive(idata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_ppc(idata, group=\"prior\", num_pp_samples=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(idata);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_ppc(idata, num_pp_samples=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use the hierarchical structure only on the slopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = {\n",
    "    \"feature\": features,\n",
    "    \"brand\": categories[\"brand\"][1],\n",
    "    \"obs_idx\": df2.index,\n",
    "}\n",
    "with pm.Model(coords=coords) as m3:\n",
    "    idx = categories[\"brand\"][0]\n",
    "    intercept = pm.Normal(\"intercept\", 6, 1, dims=\"brand\")\n",
    "    slopes_global = pm.Normal(\"slopes_global\", 0, 0.1, dims=\"feature\")\n",
    "    slopes_brand_scale = pm.HalfNormal(\"slopes_brand_scale\", 0.2)\n",
    "    slopes_brand = pm.Normal(\"slopes_brand\", 0, 0.1, dims=(\"feature\", \"brand\"))\n",
    "    slopes = slopes_global[:, None] + slopes_brand_scale * slopes_brand\n",
    "    mu = intercept[idx] + sum(\n",
    "        [\n",
    "            df2[feature].values * slopes[i, idx]\n",
    "            for i, feature in enumerate(coords[\"feature\"])\n",
    "        ],\n",
    "    )\n",
    "    sigma = pm.Exponential(\"sigma\", lam=1)\n",
    "    pm.Normal(\"log_price\", mu, sigma, observed=df2.log_selling_price, dims=\"obs_idx\")\n",
    "    idata = i3 = pm.sample(**sample_kwargs)\n",
    "    idata.extend(pm.sample_posterior_predictive(idata))\n",
    "az.plot_trace(idata);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_ppc(idata, num_pp_samples=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally have the hierarchical structure on both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = {\n",
    "    \"feature\": features,\n",
    "    \"brand\": categories[\"brand\"][1],\n",
    "    \"obs_idx\": df2.index,\n",
    "}\n",
    "with pm.Model(coords=coords) as m4:\n",
    "    idx = categories[\"brand\"][0]\n",
    "    intercept_global = pm.Normal(\"intercept_global\", 6, 1)\n",
    "    intercept_brand_scale = pm.HalfNormal(\"intercept_brand_scale\", 0.5)\n",
    "    intercept = pm.Normal(\n",
    "        \"intercept\", intercept_global, intercept_brand_scale, dims=\"brand\"\n",
    "    )\n",
    "    slopes_global = pm.Normal(\"slopes_global\", 0, 0.1, dims=\"feature\")\n",
    "    slopes_brand_scale = pm.HalfNormal(\"slopes_brand_scale\", 0.2)\n",
    "    slopes_brand = pm.Normal(\"slopes_brand\", 0, 0.1, dims=(\"feature\", \"brand\"))\n",
    "    slopes = slopes_global[:, None] + slopes_brand_scale * slopes_brand\n",
    "    mu = intercept[idx] + sum(\n",
    "        [\n",
    "            df2[feature].values * slopes[i, idx]\n",
    "            for i, feature in enumerate(coords[\"feature\"])\n",
    "        ],\n",
    "    )\n",
    "    sigma = pm.Exponential(\"sigma\", lam=1)\n",
    "    pm.Normal(\"log_price\", mu, sigma, observed=df2.log_selling_price, dims=\"obs_idx\")\n",
    "    idata = i4 = pm.sample(**sample_kwargs)\n",
    "    idata.extend(pm.sample_posterior_predictive(idata))\n",
    "az.plot_trace(idata);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_ppc(idata, num_pp_samples=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested hierarchical models\n",
    "\n",
    "To begin with, we will use the nested hierarchy for the intercept parameter and not include any hierarchical structure for the slopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = {\n",
    "    \"feature\": features,\n",
    "    \"brand\": categories[\"brand\"][1],\n",
    "    \"name\": categories[\"name\"][1],\n",
    "    \"obs_idx\": df2.index,\n",
    "}\n",
    "with pm.Model(coords=coords) as m5:\n",
    "    idx_brand = categories[\"brand\"][0]\n",
    "    idx_name = categories[\"name\"][0]\n",
    "    intercept_global = pm.Normal(\"intercept_global\", 6, 1)\n",
    "    intercept_brand = pm.Normal(\"intercept_brand\", 0, 1, dims=\"brand\")\n",
    "    intercept_brand_scale = pm.HalfNormal(\"intercept_brand_scale\")\n",
    "    intercept_name_scale = pm.HalfNormal(\"intercept_name_scale\")\n",
    "    intercept_name = pm.Normal(\n",
    "        \"intercept_name\",\n",
    "        (intercept_global + intercept_brand_scale * intercept_brand)[\n",
    "            np.array(list(name_to_brand_idx.values()))\n",
    "        ],\n",
    "        intercept_name_scale,\n",
    "        dims=\"name\",\n",
    "    )\n",
    "    slopes = pm.Normal(\"slopes\", 0, 0.1, dims=\"feature\")\n",
    "    mu = intercept_name[idx_name] + pm.math.dot(df2[coords[\"feature\"]].values, slopes)\n",
    "    sigma = pm.Exponential(\"sigma\", lam=1)\n",
    "    pm.Normal(\"log_price\", mu, sigma, observed=df2.log_selling_price, dims=\"obs_idx\")\n",
    "    idata = i5 = pm.sample(**sample_kwargs)\n",
    "    idata.extend(pm.sample_posterior_predictive(idata))\n",
    "az.plot_trace(idata);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_ppc(idata, num_pp_samples=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add the brand hierarchy of the slopes again for our full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = {\n",
    "    \"feature\": features,\n",
    "    \"brand\": categories[\"brand\"][1],\n",
    "    \"name\": categories[\"name\"][1],\n",
    "    \"obs_idx\": df2.index,\n",
    "}\n",
    "with pm.Model(coords=coords) as m6:\n",
    "    idx_brand = categories[\"brand\"][0]\n",
    "    idx_name = categories[\"name\"][0]\n",
    "    intercept_global = pm.Normal(\"intercept_global\", 6, 1)\n",
    "    intercept_brand = pm.Normal(\"intercept_brand\", 0, 1, dims=\"brand\")\n",
    "    intercept_brand_scale = pm.HalfNormal(\"intercept_brand_scale\")\n",
    "    intercept_name_scale = pm.HalfNormal(\"intercept_name_scale\")\n",
    "    intercept_name = pm.Normal(\n",
    "        \"intercept_name\",\n",
    "        (intercept_global + intercept_brand_scale * intercept_brand)[\n",
    "            np.array(list(name_to_brand_idx.values()))\n",
    "        ],\n",
    "        intercept_name_scale,\n",
    "        dims=\"name\",\n",
    "    )\n",
    "    slopes_global = pm.Normal(\"slopes_global\", 0, 0.1, dims=\"feature\")\n",
    "    slopes_brand_scale = pm.HalfNormal(\"slopes_brand_scale\", 0.2)\n",
    "    slopes_brand = pm.Normal(\"slopes_brand\", 0, 0.1, dims=(\"feature\", \"brand\"))\n",
    "    slopes = slopes_global[:, None] + slopes_brand_scale * slopes_brand\n",
    "    mu = intercept_name[idx_name] + sum(\n",
    "        [\n",
    "            df2[feature].values * slopes[i, idx]\n",
    "            for i, feature in enumerate(coords[\"feature\"])\n",
    "        ],\n",
    "    )\n",
    "    sigma = pm.Exponential(\"sigma\", lam=1)\n",
    "    pm.Normal(\"log_price\", mu, sigma, observed=df2.log_selling_price, dims=\"obs_idx\")\n",
    "    idata = i6 = pm.sample(**sample_kwargs)\n",
    "    idata.extend(pm.sample_posterior_predictive(idata))\n",
    "az.plot_trace(idata);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_ppc(idata, num_pp_samples=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section we will see that this is our best predictive model. Despite that, the model isn't great. To see that we will have to look at the per brand selling price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = np.ceil(len(categories[\"brand\"][1]) / 4)\n",
    "fig, axs = plt.subplots(int(nrows), 4, figsize=(12, 10))\n",
    "for i, (ax, brand) in enumerate(zip(axs.flatten(), categories[\"brand\"][1])):\n",
    "    az.plot_ppc(\n",
    "        idata,\n",
    "        num_pp_samples=100,\n",
    "        coords={\"obs_idx\": df2.query(f\"brand == @brand\").index},\n",
    "        ax=ax,\n",
    "    )\n",
    "    ax.set_title(brand)\n",
    "    if i // 4 < nrows - 1:\n",
    "        ax.set_xlabel(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at these posterior predictives per brand, you will notice that the brand observations do not seem normal. At most, they look like mixtures of normals, some of which have very small variance. Let's zoom in even more and look at the posterior predictive per car name inside each brand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(\n",
    "    len(categories[\"brand\"][1]), 5, figsize=(14, 25), layout=\"constrained\"\n",
    ")\n",
    "for axs_row, (brand, subdf) in zip(axs, df2.groupby(\"brand\")):\n",
    "    for ax, (name, by_name) in itertools.zip_longest(\n",
    "        axs_row, subdf.groupby(\"name\"), fillvalue=(None, None)\n",
    "    ):\n",
    "        if isinstance(ax, tuple):\n",
    "            break\n",
    "        if by_name is None:\n",
    "            ax.set_axis_off()\n",
    "        else:\n",
    "            az.plot_ppc(\n",
    "                idata,\n",
    "                num_pp_samples=100,\n",
    "                coords={\"obs_idx\": by_name.index},\n",
    "                ax=ax,\n",
    "                legend=False,\n",
    "            )\n",
    "            ax.set(title=name, xlabel=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have the key issue with all of our models so far, the observations are clearly not normally distributed. For example, TATA MANZA is made up by two price values, some of which have more occurences than the other. This is also the case for some cars from Ford. This means that our assumed observational distribution is wrong to begin with. There is two last things that we could do to improve it while still assuming normally distributed observations around some means:\n",
    "1. Make the name scale parameter change by brand. This means that the expected price of each car model from the same brand will be pulled towards the brand expected price with different strengths\n",
    "2. Make the observational noise, `sigma` be brand or name dependent. This would allow the model predictions to have different spreads per brand or car model. Implementing this in practice would not be easy given that the observations are still not normally distributed and some car models have very few data points.\n",
    "\n",
    "The real way to resolve this fundamental problem with the model is to think deeply about how to change the observational distribution and move away from a simple Gaussian. It might make sense to consider mixture models with some forms of special parametrization. We wont touch upon those though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model comparison\n",
    "\n",
    "We can use arviz to run a leave one out kind of cross validation estimate. For these models, we will get warnings regarding the shape of a Pareto distribution. This warning comes from a diagnostic statistic called $\\hat{k}$. It essentially measures the influence of the individual observations on the goodness of the estimate of LOO predictive accuracy. If we have some individual observations that are extremely influencial for the fit results, this implies that the model predictions might be very different when said observations are left out or not. In turn, this makes the approximation method used by `arviz` to be potentially biased, so a better way to approach the computation of LOO would be to actually fit the model on a subset of data without the influencial point and then compute the actual LOO score, instead of relying on the Pareto approximation that `arviz` uses by default.\n",
    "\n",
    "We won't do that here, but we will show you how to visualize the $\\hat{k}$ and see the number of problematic observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp = az.compare(\n",
    "    {\n",
    "        \"Single intercept\": i0,\n",
    "        \"Simple linear\": i1,\n",
    "        \"Brand Hier intercept\": i2,\n",
    "        \"Brand Hier slope\": i3,\n",
    "        \"Brand Hier intercept and brand Hier slope\": i4,\n",
    "        \"Brand/name Hier intercept\": i5,\n",
    "        \"Brand/name Hier intercept and brand Hier slope\": i6,\n",
    "    },\n",
    "    ic=\"loo\",\n",
    ")\n",
    "cmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6), layout=\"tight\")\n",
    "az.plot_compare(cmp);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loo = az.loo(i2, pointwise=True)\n",
    "\n",
    "az.plot_khat(loo, show_bins=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loo = az.loo(i3, pointwise=True)\n",
    "print(loo)\n",
    "\n",
    "az.plot_khat(loo, show_bins=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loo = az.loo(i4, pointwise=True)\n",
    "print(loo)\n",
    "\n",
    "az.plot_khat(loo, show_bins=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loo = az.loo(i5, pointwise=True)\n",
    "print(loo)\n",
    "\n",
    "az.plot_khat(loo, show_bins=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loo = az.loo(i6, pointwise=True)\n",
    "print(loo)\n",
    "\n",
    "az.plot_khat(loo, show_bins=True);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('bayes_course')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "485c2aecfeff35fe97c500045cb91db26354005e32990317d3834cb0213a269e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
